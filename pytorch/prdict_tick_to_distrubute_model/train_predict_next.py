import os
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from sklearn.metrics import classification_report, accuracy_score, f1_score

# ========== åƒæ•¸å€å¡Šï¼ˆå¯èª¿æ•´è¶…åƒæ•¸èˆ‡æª”æ¡ˆè·¯å¾‘ï¼‰ ==========
INPUT_DIM = 18              # æ¯å€‹ tick çš„ç‰¹å¾µç¶­åº¦
HIDDEN_DIM = 256          # Transformer çš„éš±è—å±¤ç¶­åº¦
NUM_HEADS = 4               # å¤šé ­æ³¨æ„åŠ›çš„é ­æ•¸
NUM_LAYERS = 4              # Transformer çš„å±¤æ•¸
DROPOUT = 0.1               # dropout æ©Ÿç‡
d_model = HIDDEN_DIM        # d_model åƒæ•¸
LEARNING_RATE = 5e-5        # åˆå§‹å­¸ç¿’ç‡ï¼ˆæœƒå‹•æ…‹èª¿æ•´ï¼‰
NUM_EPOCHS = 100            # æœ€å¤šè¨“ç·´ epoch æ•¸
BATCH_SIZE = 64             # æ¯æ‰¹æ¬¡æ¨£æœ¬æ•¸
PATIENCE = 100               # early stopping å®¹å¿æ¬¡æ•¸
WARMUP_STEPS = 4000         # step-wise warmup æ­¥æ•¸ï¼ˆä¸æ˜¯ epochï¼‰
TRAIN_PATH = "../../output/pt/train.pt"      # è¨“ç·´è³‡æ–™è·¯å¾‘
VAL_PATH = "../../output/pt/val.pt"          # é©—è­‰è³‡æ–™è·¯å¾‘
MODEL_SAVE_PATH = "encoder_decoder.pt"       # å„²å­˜æ¨¡å‹è·¯å¾‘
ERROR_CSV_PATH = "tickwise_prediction_error.csv"  # è¼¸å‡ºèª¤å·®è¨˜éŒ„

# ===== Positional Encodingï¼ˆæ™‚é–“åºåˆ—ç”¨ï¼‰ =====
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=512):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.pe = pe.unsqueeze(0)  # [1, max_len, d_model]

    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :].to(x.device)
        return x

# ===== æ¨¡å‹å®šç¾©ï¼šEncoder-Decoder æ¶æ§‹é æ¸¬ tick-to-tick è®ŠåŒ– =====
class EncoderDecoderTransformer(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, dropout, use_pos_encoding=True):
        super().__init__()
        self.embedding = nn.Linear(input_dim, hidden_dim)
        self.use_pos_encoding = use_pos_encoding
        if use_pos_encoding:
            self.pos_encoder = PositionalEncoding(hidden_dim)
        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dropout=dropout, batch_first=True)
        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=num_heads, dropout=dropout, batch_first=True)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)
        self.predictor = nn.Linear(hidden_dim, input_dim)

    def forward(self, src, tgt):
        src_emb = self.embedding(src)
        tgt_emb = self.embedding(tgt)
        if self.use_pos_encoding:
            src_emb = self.pos_encoder(src_emb)
            tgt_emb = self.pos_encoder(tgt_emb)
        memory = self.encoder(src_emb)
        out = self.decoder(tgt_emb, memory)
        return self.predictor(out)

# ===== åˆ†é¡æ¨¡å‹ï¼šæ ¹æ“š tick-wise loss å‘é‡é æ¸¬æ˜¯å¦ç‚ºä¸»è§’é¢¨æ ¼ =====
class TickwiseClassifier(nn.Module):
    def __init__(self, seq_len):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(seq_len, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        return torch.sigmoid(self.net(x)).squeeze(1)

# ===== è‡ªè¨‚ Datasetï¼šå°‡ [0~62] åšç‚º srcï¼Œé æ¸¬ [1~63] åšç‚º tgt =====
class SeqToSeqTickDataset(Dataset):
    def __init__(self, data_tensor):
        self.src = data_tensor['x_seq'][:, :-1, :]
        self.tgt = data_tensor['x_seq'][:, 1:, :]
        self.labels = data_tensor['y'].squeeze()  # åŠ å…¥ .squeeze() ç¢ºä¿ç‚ºä¸€ç¶­ tensor

    def __len__(self):
        return len(self.src)

    def __getitem__(self, idx):
        return self.src[idx], self.tgt[idx], self.labels[idx]

# ===== é æ¸¬èª¤å·®ï¼ˆtick-wise loss vectorï¼‰ =====
def evaluate_tickwise_loss(model, dataset, device):
    model.eval()
    loader = DataLoader(dataset, batch_size=1)
    loss_vectors = []
    labels = []
    criterion = nn.MSELoss(reduction='none')
    with torch.no_grad():
        for src, tgt, label in loader:
            src, tgt = src.to(device), tgt.to(device)
            pred = model(src, tgt)
            loss = criterion(pred, tgt).mean(dim=2)  # æ¯ tick çš„ loss å‘é‡
            loss_vectors.append(loss.squeeze(0).cpu())
            labels.append(label.item())
    return torch.stack(loss_vectors), torch.tensor(labels)

# ===== ä¸»è¨“ç·´èˆ‡é©—è­‰æµç¨‹ =====
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"âœ… ä½¿ç”¨è£ç½®ï¼š{device}")

    train_data = torch.load(TRAIN_PATH)
    val_data = torch.load(VAL_PATH)

    train_dataset = SeqToSeqTickDataset(train_data)
    val_dataset = SeqToSeqTickDataset(val_data)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

    model = EncoderDecoderTransformer(
        input_dim=INPUT_DIM,
        hidden_dim=HIDDEN_DIM,
        num_heads=NUM_HEADS,
        num_layers=NUM_LAYERS,
        dropout=DROPOUT,
        use_pos_encoding=True
    ).to(device)

    clf = TickwiseClassifier(seq_len=63).to(device)

    optimizer = torch.optim.Adam(list(model.parameters()) + list(clf.parameters()), lr=LEARNING_RATE)

    # Warmup + Cosine Annealing Scheduler (step-wise)
    total_steps = NUM_EPOCHS * len(train_loader)
    warmup_steps = WARMUP_STEPS

    def lr_lambda(step):
        if step < warmup_steps:
            return float(step) / float(max(1, warmup_steps))
        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))
        return 0.5 * (1.0 + np.cos(np.pi * progress))

    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    criterion = nn.MSELoss(reduction='none')
    bce_loss = nn.BCELoss()
    best_f1 = -1
    patience_counter = 0
    global_step = 0

    for epoch in range(NUM_EPOCHS):
        model.train()
        clf.train()
        total_loss = 0

        for src, tgt, label in train_loader:
            src, tgt, label = src.to(device), tgt.to(device), label.to(device).float().squeeze()
            pred = model(src, tgt)
            tick_loss = criterion(pred, tgt).mean(dim=2)
            pred_logits = clf(tick_loss)
            loss = bce_loss(pred_logits, label)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            scheduler.step()
            global_step += 1

            total_loss += loss.item() * src.size(0)

        avg_train_loss = total_loss / len(train_dataset)

        model.eval()
        clf.eval()
        val_total_loss = 0
        sigmoid_outputs = []
        with torch.no_grad():
            val_loss_vecs, val_labels = evaluate_tickwise_loss(model, val_dataset, device)
            pred_probs = clf(val_loss_vecs.to(device))
            preds = pred_probs > 0.5

            val_loss = bce_loss(pred_probs, val_labels.float().to(device)).item()
            sigmoid_outputs.extend(pred_probs.cpu().numpy())

            acc = accuracy_score(val_labels.numpy(), preds.cpu().numpy())
            f1 = f1_score(val_labels.numpy(), preds.cpu().numpy())

            num_pred_player = preds.sum().item()
            num_pred_nonplayer = len(preds) - num_pred_player

        current_lr = scheduler.get_last_lr()[0]

        print(f"Epoch {epoch+1}/{NUM_EPOCHS}")
        print(f"  - Train Loss: {avg_train_loss:.6f}")
        print(f"  - Val Loss: {val_loss:.6f}")
        print(f"  - Val ACC: {acc:.4f}")
        print(f"  - Val F1: {f1:.4f}")
        print(f"  - Current LR: {current_lr:.8f}")
        print(f"  - Sigmoid Avg: {np.mean(sigmoid_outputs):.4f}, Std: {np.std(sigmoid_outputs):.4f}")
        print(f"  - Predicted Players: {num_pred_player}, Non-Players: {num_pred_nonplayer}\n")

        if f1 > best_f1:
            best_f1 = f1
            patience_counter = 0
            torch.save(model.state_dict(), MODEL_SAVE_PATH)
        else:
            patience_counter += 1
            if patience_counter >= PATIENCE:
                print(f"ğŸ›‘ Early stopping triggered at epoch {epoch+1}")
                break

    print("\nğŸ§ª æœ€çµ‚æ¨è«–åˆ†æä¸­...")
    model.load_state_dict(torch.load(MODEL_SAVE_PATH))
    val_loss_vecs, val_labels = evaluate_tickwise_loss(model, val_dataset, device)
    preds = clf(val_loss_vecs.to(device)) > 0.5
    print(classification_report(val_labels.numpy(), preds.cpu().numpy(), digits=4))

if __name__ == "__main__":
    main()
