# âœ… transform_3_cls_v3.pyï¼ˆæ­£å¼ç‰ˆï¼ŒåŒ…å« acceptable normal acc + æ‡²ç½°æ©Ÿåˆ¶ï¼‰

import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import precision_score, recall_score, f1_score
import random

# ========== è¶…åƒæ•¸è¨­å®š ========== #
SEED = 47
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

# ç‰¹å¾µæ•¸é‡ã€Transformeræ¶æ§‹è¨­å®š
input_dim = 6
hidden_dim = 1024
num_heads = 8
num_layers = 4
batch_size = 64

# å„ªåŒ–å™¨ã€è¨“ç·´è¨­å®š
learning_rate = 1e-4
num_epochs = 100
patience = 10

# æå¤±å‡½æ•¸è¨­å®š
lambda_center = 0.001
acceptable_normal_acc = 0.80   # æ­£å¸¸ç©å®¶æ­£å¸¸ç‡æœ€ä½è¦æ±‚
penalty_weight = 10            # æ­£å¸¸ç‡ä¸å¤ æ™‚çš„æ‡²ç½°åŠ æ¬Š

# ========== æ¸¬è©¦é›†ç•°å¸¸ç‡åˆ†æ ========== #
def test_abnormal_ratio(model, center, threshold, test_csv, device):
    """åœ¨æ¸¬è©¦é›†ä¸Šçµ±è¨ˆæ­£å¸¸ç©å®¶èˆ‡ç•°å¸¸ç©å®¶çš„ç•°å¸¸åˆ¤å®šç‡"""
    model.eval()
    test_dataset = CustomDataset(test_csv)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    all_preds, all_labels = [], []
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            features = model(X_batch)
            distances = torch.norm(features - center, dim=1)
            preds = (distances > threshold).float()
            all_preds.append(preds.cpu())
            all_labels.append(y_batch.cpu())

    all_preds = torch.cat(all_preds)
    all_labels = torch.cat(all_labels)

    # æ­£å¸¸ç©å®¶ vs éæ­£å¸¸ç©å®¶åˆ†é–‹çµ±è¨ˆ
    normal_mask = all_labels == 1
    abnormal_mask = all_labels == 0

    normal_abnormal_rate = all_preds[normal_mask].sum().item() / max(normal_mask.sum().item(), 1)
    abnormal_abnormal_rate = all_preds[abnormal_mask].sum().item() / max(abnormal_mask.sum().item(), 1)
    normal_correct_rate = (all_preds[normal_mask] == 0).sum().item() / max(normal_mask.sum().item(), 1)

    # é¡¯ç¤ºçµæœ
    print(f"\nğŸ“Š æ¸¬è©¦é›†ä¸­ï¼š")
    print(f"â€¢ æ­£å¸¸ç©å®¶ç•°å¸¸ç‡ï¼š{normal_abnormal_rate:.2%}")
    print(f"â€¢ æ­£å¸¸ç©å®¶æ­£å¸¸ç‡ï¼š{normal_correct_rate:.2%}")
    print(f"â€¢ å…¶ä»–ç©å®¶ç•°å¸¸ç‡ï¼š{abnormal_abnormal_rate:.2%}")
    print(f"â†’ å·®è·ï¼š{(abnormal_abnormal_rate - normal_abnormal_rate):.2%}")

# ========== Transformer ç‰¹å¾µèƒå–å™¨ (åŠ CLS Token) ========== #
class TransformerFeatureExtractor(nn.Module):
    """å°‡è¼¸å…¥åºåˆ—ç¶“é Transformer èƒå–ï¼Œå– CLS token ä½œç‚ºç‰¹å¾µ"""
    def __init__(self, input_dim, num_heads, num_layers, hidden_dim, dropout_rate=0.1):
        super().__init__()
        self.embedding = nn.Linear(input_dim, hidden_dim)
        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dropout=dropout_rate, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_dim))  # åˆå§‹åŒ–ä¸€å€‹å­¸ç¿’åˆ°çš„CLS token

    def forward(self, x):
        x = self.embedding(x)
        batch_size = x.size(0)
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)  # CLSæ”¾åˆ°æœ€å‰é¢
        x = self.transformer(x)
        return x[:, 0, :]  # å–CLS tokenå°æ‡‰çš„è¼¸å‡º

# ========== è‡ªé©æ‡‰ Center Lossï¼ˆå«æ­£å¸¸ç‡è¦æ±‚ï¼‰ ========== #
class AdaptiveCenterLoss(nn.Module):
    def __init__(self, feature_dim, lambda_center=0.001, margin=0.7, penalty_abnormal_weight=30, fixed_threshold=1.0):
        super().__init__()
        self.center = nn.Parameter(torch.randn(feature_dim))
        self.lambda_center = lambda_center
        self.margin = margin
        self.penalty_abnormal_weight = penalty_abnormal_weight
        self.threshold = fixed_threshold  # å›ºå®š thresholdï¼ˆä¸æ˜¯ nn.Parameterï¼‰

    def forward(self, features, labels):
        dists = torch.norm(features - self.center, dim=1)
        preds = (dists > self.threshold).float()

        normal_mask = labels == 1
        abnormal_mask = labels == 0
        normal_total = normal_mask.sum().item()
        abnormal_total = abnormal_mask.sum().item()

        # 1. æ­£å¸¸ç‡è¨ˆç®—
        if normal_total > 0:
            normal_correct = (preds[normal_mask] == 0).sum().item()
            normal_acc = normal_correct / normal_total
        else:
            normal_acc = 1.0

        # 2. åŸºæœ¬ Lossï¼ˆè¶Šé threshold çš„æ‰æœ‰æ‡²ç½°ï¼‰
        base_loss = F.relu(dists - self.threshold) ** 2
        center_loss = base_loss.mean() + self.lambda_center * self.threshold

        # 3. å¦‚æœæ­£å¸¸ç‡ä¸å¤ ï¼ŒåŠ æ‡²ç½°
        if normal_acc < acceptable_normal_acc:
            penalty = penalty_weight * (acceptable_normal_acc - normal_acc)
            center_loss += penalty

        # 4. å¦‚æœç•°å¸¸æ¨£æœ¬é›¢ center å¤ªè¿‘ï¼ŒåŠ é‡æ‡²ç½°
        if abnormal_total > 0:
            abnormal_close = (dists[abnormal_mask] < (self.threshold - self.margin)).float().mean()
            center_loss += self.penalty_abnormal_weight * abnormal_close

        return center_loss


# ========== è‡ªè¨‚ Dataset è®€å–å™¨ ========== #
class CustomDataset(Dataset):
    def __init__(self, csv_file):
        df = pd.read_csv(csv_file, dtype=np.float32)
        self.features = df.iloc[:, :-1].values.reshape(-1, 64, 6)
        self.labels = df.iloc[:, -1].values

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return torch.tensor(self.features[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)

# ========== é©—è­‰æµç¨‹ ========== #
def validate(model, center, threshold, dataloader, device):
    model.eval()
    all_preds, all_labels = [], []
    with torch.no_grad():
        for X_batch, y_batch in dataloader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            features = model(X_batch)
            distances = torch.norm(features - center, dim=1)
            preds = (distances > threshold).float()
            all_preds.append(preds.cpu())
            all_labels.append(y_batch.cpu())

    all_preds = torch.cat(all_preds)
    all_labels = torch.cat(all_labels)

    precision = precision_score(all_labels, all_preds, zero_division=0)
    recall = recall_score(all_labels, all_preds, zero_division=0)
    f1 = f1_score(all_labels, all_preds, zero_division=0)

    normal_mask = all_labels == 1
    abnormal_mask = all_labels == 0

    normal_total = normal_mask.sum().item()
    abnormal_total = abnormal_mask.sum().item()

    normal_accuracy = (all_preds[normal_mask] == 0).sum().item() / max(normal_total, 1)
    normal_abnormal_rate = (all_preds[normal_mask] == 1).sum().item() / max(normal_total, 1)
    abnormal_abnormal_rate = (all_preds[abnormal_mask] == 1).sum().item() / max(abnormal_total, 1)
    gap = abnormal_abnormal_rate - normal_abnormal_rate

    return precision, recall, f1, normal_accuracy, normal_abnormal_rate, abnormal_abnormal_rate, gap

# ========== è¨“ç·´æµç¨‹ ========== #
def train(model, loss_fn, optimizer, train_loader, val_loader, device, num_epochs, early_stopping_patience=10):
    best_f1 = -1
    no_improve_epochs = 0

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0

        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            features = model(X_batch)
            loss = loss_fn(features, y_batch)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)

        precision, recall, f1, normal_accuracy, normal_abnormal_rate, abnormal_abnormal_rate, gap = validate(model, loss_fn.center, loss_fn.threshold, val_loader, device)

        print(f"Epoch {epoch+1}, Train Loss: {avg_loss:.4f}")
        print(f"Val Precision: {precision:.4f}, Val Recall: {recall:.4f}, Val F1: {f1:.4f}, Normal Acc: {normal_accuracy:.4f}")
        print(f"â€¢ æ­£å¸¸ç©å®¶ç•°å¸¸ç‡ï¼š{normal_abnormal_rate:.2%} | å…¶ä»–ç©å®¶ç•°å¸¸ç‡ï¼š{abnormal_abnormal_rate:.2%} | å·®è·ï¼š{gap:.2%}")
        print(f"Threshold: {loss_fn.threshold:.4f}")

        # é–¥å€¼èª¿æ•´ï¼šç›®æ¨™æ˜¯è®“æ­£å¸¸ç©å®¶ç•°å¸¸ç‡ä¿æŒåˆç†
        # é–¥å€¼èª¿æ•´

        if f1 > best_f1:
            best_f1 = f1
            no_improve_epochs = 0
            torch.save(model.state_dict(), "best_model.pt")
            print("âœ… å„²å­˜æ–°çš„æœ€ä½³æ¨¡å‹ï¼")
        else:
            no_improve_epochs += 1
            if no_improve_epochs >= early_stopping_patience:
                print(f"ğŸ›‘ æ—©åœï¼šVal F1æ²’é€²æ­¥{early_stopping_patience}å€‹epoch")
                break

# ========== ä¸»ç¨‹å¼ ========== #
if __name__ == "__main__":
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    train_dataset = CustomDataset("../output/split/train_set.csv")
    val_dataset = CustomDataset("../output/split/val_set.csv")

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    test_dataset = CustomDataset("../output/split/test_set.csv")

    print("\nğŸ“¦ è³‡æ–™é‡çµ±è¨ˆ")
    print("===================")
    print(f"â€¢ è¨“ç·´è³‡æ–™ç¸½ç­†æ•¸ï¼š{len(train_dataset)}")
    print(f"â€¢ é©—è­‰è³‡æ–™ç¸½ç­†æ•¸ï¼š{len(val_dataset)}")
    print(f"â€¢ æ¸¬è©¦è³‡æ–™ç¸½ç­†æ•¸ï¼š{len(test_dataset)}")
    print("===================\n")

    model = TransformerFeatureExtractor(input_dim, num_heads, num_layers, hidden_dim).to(device)
    loss_fn = AdaptiveCenterLoss(
    feature_dim=hidden_dim,
    lambda_center=lambda_center,
    margin=0.7,
    penalty_abnormal_weight=30,
    fixed_threshold=1.0  # â­é€™è£¡è¨­å®šä½ çš„å›ºå®š threshold å€¼
    ).to(device)

    optimizer = optim.AdamW(list(model.parameters()) + list(loss_fn.parameters()), lr=learning_rate)

    train(model, loss_fn, optimizer, train_loader, val_loader, device, num_epochs=num_epochs, early_stopping_patience=patience)

    print("\nğŸ§ª é–‹å§‹æ¸¬è©¦é›†åˆ†æ...")
    test_abnormal_ratio(model, loss_fn.center, loss_fn.threshold, "../output/split/test_set.csv", device)
